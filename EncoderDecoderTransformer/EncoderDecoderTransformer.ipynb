{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Arithmetic with Language Models: from Memorization to Computation**\n",
    "\n",
    "This notebook facilitates the replication of experimental work on **encoder-decoder Transformer** architecture featured in [D. Maltoni and M. Ferrara, *\"Arithmetic with language models: From memorization to computation\"*, Neural Networks, vol. 179, 2024](https://www.sciencedirect.com/science/article/pii/S089360802400474X). It uses the following Python scripts:\n",
    "- **ArithmeticData.py** - contains functions to create, shuffle and split datasets used in the experimentation.\n",
    "- **Transformer.py** - contains a modified version of [\n",
    "A. Sarkar, *\"Build your own Transformer from scratch using Pytorch\"*, Towards Data Science, 2023](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb) which implements the encoder-decoder Transformer architecture introduced by [Vaswani et al. (2017)](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).\n",
    "- **TransformerUtilites.py** - provides utility functions for evaluating performance indicators.\n",
    "- **TransformerTraining.py** - contains the *transformer_training* function allowing multiple training executions (*run_count*) with the same number of epochs (**epochs**) and using the same dataset (internally created from the *op*, *revert_bit* and *val_set_type* parameters).\n",
    "- **TransformerComputeTokenAndValueDist.py** - contains the *transformer_compute_token_and_value_dist* function to study the Transformer internal representation (embedding) by correlating the distances between the embeddings and the corresponding distances at input/output levels.\n",
    "\n",
    "The following code imports all necessary modules and functions required for running this notebook. Subsequent code cells operate independently and may be run in any sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from TransformerTraining import transformer_training\n",
    "from TransformerComputeTokenAndValueDist import transformer_compute_token_and_value_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the left graph reported in Figure 1. To store the weights of the trained models, specify the *out_folder_path* parameter with the directory path where you want the models to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 1 (left)\n",
    "\n",
    "op='+'\n",
    "run_count=5\n",
    "epochs=50\n",
    "out_folder_path=None\n",
    "\n",
    "avg_train_seq_acc,avg_val_seq_acc=transformer_training(op,run_count,epochs,out_folder_path=out_folder_path)\n",
    "\n",
    "plt.plot(avg_train_seq_acc*100,label='Train')\n",
    "plt.plot(avg_val_seq_acc*100,label='Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the right graph reported in Figure 1. To store the weights of the trained models, specify the *out_folder_path* parameter with the directory path where you want the models to be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 1 (right)\n",
    "\n",
    "op='x'\n",
    "run_count=5\n",
    "epochs=250\n",
    "out_folder_path=None\n",
    "\n",
    "avg_train_seq_acc,avg_val_seq_acc=transformer_training(op,run_count,epochs,out_folder_path=out_folder_path)\n",
    "\n",
    "plt.plot(avg_train_seq_acc*100,label='Train')\n",
    "plt.plot(avg_val_seq_acc*100,label='Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the graph reported in Figure 2. Since the trend remains consistent across multiple runs, to reduce the time needed for running the experiment, the *run_count* parameter has been set to 1. Furthermore, the *epochs* parameter has been updated to 4000 in place of the originally stated 1000, to correct an error in the x-axis label of Figure 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 2\n",
    "\n",
    "op='R'\n",
    "run_count=1\n",
    "epochs=4000\n",
    "\n",
    "avg_train_seq_acc,avg_val_seq_acc=transformer_training(op,run_count,epochs)\n",
    "\n",
    "plt.plot(avg_train_seq_acc*100,label='Train')\n",
    "plt.plot(avg_val_seq_acc*100,label='Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the left graph reported in Figure 3. To reduce the duration required for the experiment execution, adjust the *run_count* parameter to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 3 (left)\n",
    "\n",
    "op='+'\n",
    "run_count=5\n",
    "epochs=50\n",
    "\n",
    "_,avg_rndval_seq_acc=transformer_training(op,run_count,epochs)\n",
    "_,avg_vst_seq_acc=transformer_training(op,run_count,epochs,val_set_type='VSt')\n",
    "_,avg_vsv_seq_acc=transformer_training(op,run_count,epochs,val_set_type='VSv')\n",
    "\n",
    "plt.plot(avg_rndval_seq_acc*100,label='Random Split')\n",
    "plt.plot(avg_vst_seq_acc*100,label='VS_t')\n",
    "plt.plot(avg_vsv_seq_acc*100,label='VS_v')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the right graph reported in Figure 3. To reduce the duration required for the experiment execution, adjust the *run_count* parameter to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 3 (right)\n",
    "\n",
    "op='x'\n",
    "run_count=5\n",
    "epochs=250\n",
    "\n",
    "_,avg_rndval_seq_acc=transformer_training(op,run_count,epochs)\n",
    "_,avg_vst_seq_acc=transformer_training(op,run_count,epochs,val_set_type='VSt')\n",
    "_,avg_vsv_seq_acc=transformer_training(op,run_count,epochs,val_set_type='VSv')\n",
    "\n",
    "plt.plot(avg_rndval_seq_acc*100,label='Random Split')\n",
    "plt.plot(avg_vst_seq_acc*100,label='VS_t')\n",
    "plt.plot(avg_vsv_seq_acc*100,label='VS_v')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the table reported in Figure 4.a. To load previously saved weights, set the *model_checkpoint_path* parameter to the file path of the stored weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 4 (a)\n",
    "\n",
    "op='+'\n",
    "model_checkpoint_path=os.path.abspath('')+r'\\add_model'\n",
    "\n",
    "transformer_compute_token_and_value_dist(op,model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the table reported in Figure 4.b. To load previously saved weights, set the *model_checkpoint_path* parameter to the file path of the stored weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure 4 (b)\n",
    "\n",
    "op='x'\n",
    "model_checkpoint_path=os.path.abspath('')+r'\\mul_model'\n",
    "\n",
    "transformer_compute_token_and_value_dist(op,model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the left graph reported in Figure C.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure C.6 (left)\n",
    "\n",
    "op='+'\n",
    "run_count=1\n",
    "epochs=150\n",
    "\n",
    "_,avg_val_seq_acc=transformer_training(op,run_count,epochs)\n",
    "_,avg_val_seq_acc_not_rev=transformer_training(op,run_count,epochs,revert_bit=False)\n",
    "\n",
    "plt.plot(avg_val_seq_acc*100,label='Reverse')\n",
    "plt.plot(avg_val_seq_acc_not_rev*100,label='Plain')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell reproduces the experiment used to generate the right graph reported in Figure C.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure C.6 (right)\n",
    "\n",
    "op='x'\n",
    "run_count=1\n",
    "epochs=1500\n",
    "\n",
    "_,avg_val_seq_acc=transformer_training(op,run_count,epochs)\n",
    "_,avg_val_seq_acc_not_rev=transformer_training(op,run_count,epochs,revert_bit=False)\n",
    "\n",
    "plt.plot(avg_val_seq_acc*100,label='Reverse')\n",
    "plt.plot(avg_val_seq_acc_not_rev*100,label='Plain')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sequence Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
