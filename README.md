# Arithmetic with Language Models: from Memorization to Computation

This is the official implementation of the experiments conducted in **[[1]](https://www.sciencedirect.com/science/article/pii/S089360802400474X)**.

# Overview

The `EncoderDecoderTransformer` directory includes all necessary files to replicate the experiments conducted using the **encoder-decoder Transformer** architecture introduced in **[[2]](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)**.

# How to run
Clone the repository
```bash
git clone https://github.com/MatteoFerrara/Arithmetic-with-Language-Models-from-Memorization-to-Computation.git
 ```   

Execute the `EncoderDecoderTransformer.ipynb` Jupyter notebook.

# Citation
Please cite [1] in all publications and works that use this code.

# Bibliography
[1] D. Maltoni, and M. Ferrara, "Arithmetic with language models: From memorization to computation", Neural Networks, vol. 179, 2024.

[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, "Attention is all you need", Advances in neural information processing systems, vol. 30, 2017.
